{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Links between authors\n",
    "\n",
    "In this notebook we will perform a basic analysis of the links between **authors** included in the Sefaria dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pathlib\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dirname = \"../sample_dataset/\"\n",
    "raw_subdirname = \"raw/\"\n",
    "raw_metadata_subdirame = \"_schemas/\"\n",
    "links_count_fn = dataset_dirname + raw_subdirname + 'links/links_by_book_without_commentary.csv'\n",
    "output_subfolder = \"./links_files/\""
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Constants\n",
    "\n",
    "Names of books we will want to merge in the analysis:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masekhtot = ['Arakhin', 'Bekhorot', 'Chullin', 'Keritot', 'Meilah', 'Menachot', 'Tamid', 'Temurah', 'Zevachim', 'Beitzah', 'Chagigah', 'Eruvin', 'Megillah', 'Moed Katan', 'Pesachim', 'Rosh Hashanah', 'Shabbat', 'Sukkah', 'Taanit', 'Yoma', 'Gittin', 'Ketubot', 'Kiddushin', 'Nazir', 'Nedarim', 'Sotah', 'Yevamot', 'Avodah Zarah', 'Bava Batra', 'Bava Kamma', 'Bava Metzia', 'Horayot', 'Makkot', 'Sanhedrin', 'Shevuot', 'Niddah', 'Berakhot']\n",
    "\n",
    "torah_books = ['Deuteronomy', 'Exodus', 'Genesis', 'Leviticus', 'Numbers']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Threshold parameter for display of links in graph:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_count_threshold = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Authors List\n",
    "\n",
    "We read the metadata files from the dataset. Every book's metadata is in a JSON file named after the book (after spaces are replaced by underscores). We load only a list of book/author from the files.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_dict = {}\n",
    "error_count=0\n",
    "metadata_dir = dataset_dirname + raw_subdirname + raw_metadata_subdirame"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We loop over JSON files and extract author.\n",
    "\n",
    "In some cases there is no \"author\" field in the JSON. For example: anonymous books (the author is unknown), eponymous books (the name of the author is in the title of the book). In thoses case we extract the author name from the book title, since often in Jewish Thought an author is nicknamed after his major creation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metadata_fn in os.listdir(metadata_dir):\n",
    "    filename, file_extension = os.path.splitext(metadata_fn)\n",
    "    if file_extension != '.json':\n",
    "        continue\n",
    "    with open(metadata_dir+metadata_fn, 'r', encoding=\"utf8\") as metadata_file:\n",
    "        try:\n",
    "            metadata = json.load(metadata_file)\n",
    "        except:\n",
    "            continue\n",
    "    bookname = filename.replace('_', ' ')\n",
    "    try:\n",
    "        author = metadata['authors'][0]['en']\n",
    "    except:\n",
    "        if bookname.find(\" on \")>0:\n",
    "            author = bookname[0:idx]\n",
    "        elif bookname.find(\" on \")>0:\n",
    "\n",
    "        else:\n",
    "            author = bookname\n",
    "        error_count+=1\n",
    "    authors_dict[bookname] = author\n",
    "print(str(error_count) + ' books out of '+ str(len(os.listdir(metadata_dir))) +' without valid author information were corrected.')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Convert to pandas dataframe and display preview:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_authors_df = pd.DataFrame.from_dict(authors_dict, columns=['Author'], orient='index')\n",
    "book_authors_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load links count data\n",
    "\n",
    "We load the list of all (known) links between books from a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_links_counts = pd.read_csv(links_count_fn)\n",
    "all_link_counts_filtered = all_links_counts[all_links_counts['Link Count']>=0]"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Let's display some preview of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_link_counts_filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build graph\n",
    "\n",
    "We join the link counts list with the author list and keep only the \"authors\" and \"link count\" columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = all_link_counts_filtered.join(book_authors_df, on='Text 1', rsuffix='_1')\n",
    "df2 = df1.join(book_authors_df, on='Text 2', rsuffix='_2')\n",
    "df2.rename(columns={'Author': 'Author_1'}, inplace=True)\n",
    "authors_links_count = df2.loc[:,['Author_1', 'Author_2', 'Link Count']]\n",
    "authors_links_count.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We merge some books which are port of one single corpus (Torah and Talmud) and remove the Jastrow dictionnary, since links from the Jastrow only mean this is a really exhaustive dictionnary:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_links_count.replace(masekhtot, 'Talmud', inplace=True)\n",
    "authors_links_count.replace(torah_books, 'Torah', inplace=True)\n",
    "idx2 = ~((authors_links_count['Author_2']=='Marcus Jastrow') | (authors_links_count['Author_1']=='Marcus Jastrow'))\n",
    "authors_links_count = authors_links_count.loc[idx2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also remove self-references:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = ~authors_links_count['Author_1'].eq(authors_links_count['Author_2'])\n",
    "authors_links_count = authors_links_count.loc[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate identical rows and keep only links above threshold and save to file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_links_count_agg = authors_links_count.groupby(['Author_1', 'Author_2']).sum()\n",
    "authors_links_count_agg = authors_links_count_agg.reset_index() #or not?\n",
    "authors_links_count_agg.to_hdf(output_subfolder+\"whole_graph.h5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of connections to reference corpuses\n",
    "\n",
    "We want to vizualise major influences, certainly from reference sources common to most authors. To this end we keep only links above our defined threshold. Moreover, in order to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_links_count_agg_morethan = authors_links_count_agg.loc[authors_links_count_agg['Link Count']>link_count_threshold]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}